#### Terminology

Kafka cluster is made up of multiple **brokers**
Broker: server that stores data and serves clients
Broker has n **partitions** - Each partition is ordered, immutable sequence of messages that is continually appended to -> way to scale data
Topic - logical grouping of partitions -> way to organize data
Producers - write data to topics
Consumers - read data from topics
Consumer group - Each event is guaranteed to only be processed by one consumer in group
Kafka is either message queue or stream
- Message queue - consumers read and acknowledge they processed message
- Stream - consumer process message without acknowledgement

#### How Kafka Works

- When event occurs, producer formats message and sends to Kafka topic
	- Message = one required field, value, three optional fields (key, timestamp, headers)
- Key determines which partition the message is sent to 
	1. Partition Determination
		- Hash message key to assign message to specific partition
	2. Broker Assignment
		- Identify which broker holds that particular partition
		- Mapping of partitions is managed by Kafka cluster metadata
- Each partition is an append-only log file or "distributed commit log" 
- Benefits of append-only:
	1. Immutability - messages cannot be altered or deleted
		- Performance, reliability, no consistency issues
		- Simplified replication, speeds up recovery process
	2. Efficiency
		- Append only -> minimized disk seek times which bottleneck many storage systems
	3. Scalability
		- Simplicity of append-only facilitates horizontal scaling 
			- aka just add more partitions and more brokers
- Each message assigned a unique offset, identifying message's position in partition
	- Consumers maintain their current offset to track their progress in reading messages from topic, periodically commit this offset back to Kafka
- Leader-Follow Model for Replication:
1. Leader Replica Assignment
	- Each partition has leader replica - handles read and write requests
	- Cluster controller manages assignment of leader replica, making sure each partition's leader replica is distributed across cluster to balance load
2. Follower Replication
	- Followers do not handle direct client requests, but passively replicate data from leader
3. Synchronization and Consistency
	- Followers continuously sync with leader replica to ensure up to date data
	- If leader replica fails, a follower will be promoted to leader to minimize downtime/data loss
4. Controller's Role in Replication
	- Controller monitors health of all brokers, manages leadership/replication

- Consumers read messages from Kafka topics in 2 ways:
1. push model: subscribe to topic -> receive messages as they arrive
2. pull model: poll Kafka for new messages at regular intervals

When to use Kafka as:
1. Message queue
- Processing that can be done asynchronously
	- ex. Youtube: When users upload video, we make standard definition available immediately, and then put the video link in a Kafka topic to be transcoded later
- Messages must be processed in order
	- ex. Ticketmaster: users are let into booking page in order they arrived
- Need to decouple producer and consumer so they scale independently
	- Usually when producer produces messages faster than consumer consumes, so one service can't take down another
1. Stream
- Require continuous and immediate processing of incoming data -> real-time flow
	- ex. Ad Click Aggregator: aggregate click data in real-time
- Messages need to be processed by multiple consumers simultaneously
	- ex. FB Live Comments: use Kafka as pub/sub system

#### For System Design Interviews
##### Scalability
- No hard limit on size of Kafka message
- But recommended to keep messages under 1 MB for better performance via reduced memory pressure and better network utilization
- Anti-pattern to store large blobs of data in Kafka
	- Ex. Youtube: store videos in S3 and place message in Kafka with location of video in S2 rather than full video
How to Scale?
1. Horizontal Scaling with More Brokers:
- Distributes load, greater fault tolerance
2. Partitioning Strategy
- Choose a key -> partitions determined by a consistent hashing algorithm on the key
	- Good keys evenly distribute across partitions, bad keys cause hot partitions
##### How can we handle hot partitions
1. Random partitioning with no key
	- Kafka auto assigns partition to message to guarantee even distribution
	- Con: you lose ability to guarantee order of messages
- Good option if order doesn't matter
2. Random salting
	- Add random number or timestamp to ID when generating partition key
	- Con: complicates aggregation logic later on consumer side
3. Compound key
	- Combine ad ID and another attribute like geographical region or user ID
	- Useful if we can identify attributes that vary independently of ad ID
4. Back pressure
	- Just slow down producer

##### Fault Tolerance and Durability
- Data durability ensured by replication mechanism -> data available even if a broker fails
	- Producer acknowledgments (`acks=all` setting ensures all replicas must receive message before it is acknowledged)
- Replication factor: number of replicas maintained per partition
	- Factor of 3 is common
- Unlikely for Kafka to go down: "always available, sometimes consistent" 
What happens when consumer goes down?
1. Offset Management: 
	- Consumers commit their offsets to Kafka after processing message
	- When consumer restarts, it reads its last committed offset from Kafka, then resume processing from that offset
	- Trade-off: When to commit offsets
		- Keep work of consumer small so if consumer fails, we do not redo so much work
2. Rebalancing:
	- If one consumer in consumer group goes down, Kafka will redistribute partitions among remaining consumers so that all partitions are still being processed

##### Handling Retries and Errors
Producer Retries
- May fail to get message to Kafka in first place due to network issues, broker unavailability, or transient failures
- Handle this in config settings
	- Number of automatic `retries: 5`
	- `Idempotent: true` -> ensures messages only sent once, not duplicated
Consumer Retries
- Retry and dead letter queue pattern (out of box for AWS SQS, but not Kafka)
	- Set up custom topic that we can move failed messages to, and have separate consumer process these messages
	- Retry these messages n times
	- If we retry too many times, move it to dead letter queue (DLQ) -> just place to store failed messages to investigate later

##### Performance Optimizations
1. Choose better partition key (biggest impact): maximizes parallelism by ensure messages are evenly distributed
2. Batch messages in producer before sending to Kafka via config
3. Compress messages on producer via config (via GZIP, Snappy, LZ4 algos)
##### Retention Policies
- Configured in settings for how long to retain messages (default is 7 days, or until log is 1 GB)
	- Can keep messages longer but think of impact on storage costs/performance
